{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcQ-rDATOZSi"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_wTGsnYTtTc"
      },
      "outputs": [],
      "source": [
        "!pip install dtaidistance\n",
        "!pip install tensorflow==2.15.0\n",
        "!pip install tensorflow_probability==0.23.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcL84BzK0c7l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm_notebook as tqdm\n",
        "import os\n",
        "from dtaidistance import dtw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1km_m4E90ndz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p67TmJlOgrM"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jByi0jTMBTrx"
      },
      "outputs": [],
      "source": [
        "halt_at_end = True\n",
        "save_model = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5onVIMlbvYrV"
      },
      "outputs": [],
      "source": [
        "t_days = 5 # Lead time (in days)\n",
        "base_carrington = 27.26"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm0UPvK9Ojvs"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxhNzsHZdb_p"
      },
      "outputs": [],
      "source": [
        "!rm -rf ch_data\n",
        "!mkdir ch_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKc5avj9dd-R"
      },
      "outputs": [],
      "source": [
        "!unzip 2012.zip -d ch_data\n",
        "!unzip 2013.zip -d ch_data\n",
        "!unzip 2014.zip -d ch_data\n",
        "!unzip 2015.zip -d ch_data\n",
        "!unzip 2016.zip -d ch_data\n",
        "!unzip 2017.zip -d ch_data\n",
        "!unzip 2018.zip -d ch_data\n",
        "!unzip 2019.zip -d ch_data\n",
        "!unzip 2020.zip -d ch_data\n",
        "!unzip 2021.zip -d ch_data\n",
        "!unzip 2022.zip -d ch_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q_TuhVwdhrh"
      },
      "outputs": [],
      "source": [
        "from textwrap import fill\n",
        "path = 'ch_data/features/ch_measurements'\n",
        "buffer_size = 4\n",
        "data = []\n",
        "for file in os.listdir(path):\n",
        "  try:\n",
        "    d = file.split('.')\n",
        "    d = d[0]+'-'+d[1]+'-'+d[2]\n",
        "    feats = pd.read_csv(os.path.join(path, file))\n",
        "    cols = feats.columns.values\n",
        "    cols[0] = 'ID'\n",
        "    feats.columns = cols\n",
        "    feats = feats.drop(columns=['ID'])\n",
        "    cols = feats.columns\n",
        "    n_holes = buffer_size\n",
        "    new_cols = np.stack([cols for i in range(n_holes)])\n",
        "    for i in range(len(new_cols)):\n",
        "      for c in range(len(new_cols[i])):\n",
        "        new_cols[i][c] = new_cols[i][c]+'_'+str(i)\n",
        "    new_cols = new_cols.flatten()\n",
        "    vals = np.concatenate([feats.values.flatten(), [0 for _ in range(n_holes*len(cols)-len(feats.values.flatten()))]]).reshape((1, n_holes*len(cols)))\n",
        "    final_feats = pd.DataFrame(vals, columns=new_cols, index=[pd.to_datetime(d)])\n",
        "    data.append(final_feats)\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "ch_data = pd.concat(data).sort_index()\n",
        "new_idx = pd.date_range(ch_data.index[0], ch_data.index[-1], freq='1d')\n",
        "ch_data = ch_data.reindex(new_idx, fill_value=0)\n",
        "new_idx = pd.date_range(ch_data.index[0], ch_data.index[-1], freq='1h')\n",
        "ch_data = ch_data.reindex(new_idx)\n",
        "drop_list = []\n",
        "for c in ch_data.columns:\n",
        "  if not('area' in c) and not('top' in c) and not('bot' in c) and not('left' in c) and not('right' in c) and not('flux' in c):\n",
        "    drop_list.append(c)\n",
        "  if 'flux' in c:\n",
        "    ch_data[c] = ch_data[c].apply(np.abs)\n",
        "ch_data=ch_data.drop(columns=drop_list)\n",
        "for c in ch_data.columns:\n",
        "  ch_data[c] = ch_data[c].astype(np.float32)\n",
        "ch_data = ch_data.interpolate(method='time')\n",
        "ch_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRaHFAA-0uhq"
      },
      "outputs": [],
      "source": [
        "wind_data_path = 'drive/MyDrive/SpaceWeatherData/ACE/omni_1hr_clean.csv'\n",
        "wind_data = pd.read_csv(wind_data_path, parse_dates=[0], index_col=0).interpolate(method='time')\n",
        "wind_data['B'] = np.sqrt(wind_data['BR']**2+wind_data['BT']**2+wind_data['BN']**2)\n",
        "wind_data['T'] = np.log10(wind_data['T'])\n",
        "wind_data['N'] = np.log10(wind_data['N'])\n",
        "wind_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWi2ijLndpaq"
      },
      "outputs": [],
      "source": [
        "wind_data = ch_data.join(wind_data)\n",
        "wind_data = wind_data.join(wind_data.shift(round(24*(base_carrington-t_days))), rsuffix='_Car').dropna(how='any')\n",
        "wind_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxIrGIcw1Bjj"
      },
      "outputs": [],
      "source": [
        "in_cols = ['V', 'N', 'T', 'B', 'V_Car', 'N_Car', 'T_Car', 'B_Car']+ch_data.columns.tolist()+(ch_data.columns.values+'_Car').tolist()\n",
        "out_cols = ['V']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHVtafaD1M26"
      },
      "outputs": [],
      "source": [
        "in_mu = wind_data[in_cols].mean().values\n",
        "in_std = wind_data[in_cols].std().values\n",
        "out_mu = wind_data[out_cols].mean().values\n",
        "out_std = wind_data[out_cols].std().values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXVNquUX1caK"
      },
      "outputs": [],
      "source": [
        "def standardize(x, inpt=True):\n",
        "  if inpt:\n",
        "    return (x-in_mu)/in_std\n",
        "  else:\n",
        "    return (x-out_mu)/out_std\n",
        "\n",
        "def destandardize(x, inpt=True):\n",
        "  if inpt:\n",
        "    return x*in_std+in_mu\n",
        "  else:\n",
        "    return x*out_std+out_mu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVI5X8Je1zUT"
      },
      "outputs": [],
      "source": [
        "in_size= round(24*5)\n",
        "offset = round(24*t_days)\n",
        "batch_size = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKjgeG_YOpu8"
      },
      "source": [
        "## Dataset generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BZU1uj_2ADS"
      },
      "outputs": [],
      "source": [
        "idx = wind_data.index.values\n",
        "l = len(idx)\n",
        "\n",
        "train_p = 0.7\n",
        "train_portion = idx[:round(train_p*l)]\n",
        "val_portion = idx[round(train_p*l):round(((1-train_p)/2+train_p)*l)]\n",
        "test_portion = idx[round(((1-train_p)/2+train_p)*l):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fp-2Ab6sH-wA"
      },
      "outputs": [],
      "source": [
        "display(train_portion)\n",
        "display(len(train_portion))\n",
        "print()\n",
        "display(val_portion)\n",
        "display(len(val_portion))\n",
        "print()\n",
        "display(test_portion)\n",
        "display(len(test_portion))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5-FLYEO2gYC"
      },
      "outputs": [],
      "source": [
        "train_set = keras.utils.timeseries_dataset_from_array(\n",
        "    standardize(wind_data.loc[train_portion[0]:train_portion[-1], in_cols][:-offset]),\n",
        "    standardize(wind_data.loc[train_portion[0]:train_portion[-1], out_cols][offset:], inpt=False),\n",
        "    start_index=offset, sequence_length=in_size, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_set = keras.utils.timeseries_dataset_from_array(\n",
        "    standardize(wind_data.loc[val_portion[0]:val_portion[-1], in_cols][:-offset]),\n",
        "    standardize(wind_data.loc[val_portion[0]:val_portion[-1], out_cols][offset:], inpt=False),\n",
        "    start_index=offset, sequence_length=in_size, batch_size=batch_size)\n",
        "\n",
        "test_set = keras.utils.timeseries_dataset_from_array(\n",
        "    standardize(wind_data.loc[test_portion[0]:test_portion[-1], in_cols][:-offset]),\n",
        "    standardize(wind_data.loc[test_portion[0]:test_portion[-1], out_cols][offset:], inpt=False),\n",
        "    start_index=offset, sequence_length=in_size, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF_F_S8yOsoy"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9Zso5Ux3dr6"
      },
      "outputs": [],
      "source": [
        "use_exact_kl = False\n",
        "\n",
        "def prior(kernel_size, bias_size, dtype=None):\n",
        "    delimiters = [400, 600, 700, 800]\n",
        "    prop = len(wind_data[wind_data.V < delimiters[0]])/len(wind_data)\n",
        "    props = [prop]\n",
        "    pstds = [standardize(wind_data[wind_data.V < delimiters[0]][in_cols].std(), inpt=False)[0]]\n",
        "    pmus = [standardize(wind_data[wind_data.V < delimiters[0]][in_cols].mean(), inpt=False)[0]]\n",
        "    for i in range(1, len(delimiters)-1):\n",
        "      d, dp = delimiters[i], delimiters[i+1]\n",
        "      aux = wind_data[wind_data.V >= d]\n",
        "      aux = aux[aux < dp]\n",
        "      props.append(len(aux)/len(wind_data))\n",
        "      pstds.append(standardize(aux[in_cols].std(), inpt=False)[0])\n",
        "      pmus.append(standardize(aux[in_cols].mean(), inpt=False)[0])\n",
        "      if np.isnan(pstds[-1]):\n",
        "        pstds[-1] = pstds[-2]\n",
        "      if np.isnan(pmus[-1]):\n",
        "        pmus[-1] = pmus[-2]\n",
        "    props.append(len(wind_data[wind_data.V >= delimiters[-1]])/len(wind_data))\n",
        "    pstds.append(standardize(wind_data[wind_data.V >= delimiters[-1]][in_cols].std(), inpt=False)[0])\n",
        "    pmus.append(standardize(wind_data[wind_data.V >= delimiters[-1]][in_cols].mean(), inpt=False)[0])\n",
        "    if np.isnan(pstds[-1]):\n",
        "        pstds[-1] = pstds[-2]\n",
        "    if np.isnan(pmus[-1]):\n",
        "      pmus[-1] = pmus[-2]\n",
        "    n = kernel_size + bias_size\n",
        "    prior_model = keras.Sequential(\n",
        "        [\n",
        "            tfp.layers.DistributionLambda(\n",
        "                lambda t:\n",
        "                tfp.distributions.Mixture(\n",
        "                    cat=tfp.distributions.Categorical(probs=props),\n",
        "                    components=[\n",
        "                    tfp.distributions.MultivariateNormalDiag(\n",
        "                        loc=tf.zeros(n)+pmus[i], scale_diag=tf.ones(n)*pstds[i]\n",
        "                    ) for i in range(len(props))\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    return prior_model\n",
        "\n",
        "def posterior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "    posterior_model = keras.Sequential(\n",
        "        [\n",
        "            tfp.layers.VariableLayer(\n",
        "                tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype\n",
        "            ),\n",
        "            tfp.layers.MultivariateNormalTriL(n),\n",
        "        ]\n",
        "    )\n",
        "    return posterior_model\n",
        "\n",
        "def negative_loglikelihood(targets, estimated_distribution):\n",
        "  return -estimated_distribution.log_prob(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKFJv49l3iYx"
      },
      "outputs": [],
      "source": [
        "hidden_units = [60]\n",
        "\n",
        "def build_model(train_size):\n",
        "  x=in_layer = keras.layers.Input((in_size, len(in_cols)))\n",
        "  x = keras.layers.LSTM(len(in_cols), return_sequences=False, activation='tanh')(x)\n",
        "  features=x\n",
        "  for units in hidden_units:\n",
        "        features = tfp.layers.DenseVariational(\n",
        "            units=units,\n",
        "            make_prior_fn=prior,\n",
        "            make_posterior_fn=posterior,\n",
        "            kl_weight=1 / train_size,\n",
        "            activation= \"tanh\",\n",
        "            kl_use_exact = use_exact_kl\n",
        "        )(features)\n",
        "  distribution_params = keras.layers.Dense(units=2)(features)\n",
        "  outputs = tfp.layers.IndependentNormal(1)(distribution_params)\n",
        "\n",
        "  model = keras.Model(inputs=in_layer, outputs=outputs)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuVQX2_ROxJ1"
      },
      "source": [
        "## Model fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i34IaQwe4UWy"
      },
      "outputs": [],
      "source": [
        "model = build_model(len(train_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G3UpNPM4YLC"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFp6JrX_4eqq"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEKnZsBV4l9G"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=negative_loglikelihood,\n",
        "              optimizer=keras.optimizers.RMSprop(clipnorm=1.0),\n",
        "              metrics=['mse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzmMutT04p-a"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "    keras.callbacks.TerminateOnNaN()\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNaV_Ec-4z56"
      },
      "outputs": [],
      "source": [
        "epochs = 3000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdWIjWhV42Fq"
      },
      "outputs": [],
      "source": [
        "model.fit(train_set, validation_data = val_set, epochs=epochs,\n",
        "          callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjrFqc2kO2im"
      },
      "source": [
        "## Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Omp59r7o5I1L"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCSUoE7WQZ2V"
      },
      "outputs": [],
      "source": [
        "t = out_cols[0]\n",
        "units = {'V': '$\\mathrm{km}\\,\\mathrm{s}^{-1}$',\n",
        "         'T': 'K',\n",
        "         'N': 'cm$^{-3}$',\n",
        "         'B': 'nT',\n",
        "         'BR': 'nT',\n",
        "         'BN': 'nT',\n",
        "         'BT': 'nT'}\n",
        "\n",
        "t_names = {\n",
        "    'V': 'V',\n",
        "    'T': 'T',\n",
        "    'N': '\\\\rho',\n",
        "    'B': '|\\mathbf{B}|',\n",
        "    'BR': '-B_x',\n",
        "    'BT': 'B_y',\n",
        "    'BN': 'B_z'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAuE44X_0n0X"
      },
      "outputs": [],
      "source": [
        "def mse(a, b):\n",
        "  return np.mean((a-b)**2)\n",
        "\n",
        "def mae(a, b):\n",
        "  return np.mean(np.abs(a-b))\n",
        "\n",
        "def rmse(a, b):\n",
        "  return np.sqrt(mse(a, b))\n",
        "\n",
        "def cc(a, b):\n",
        "  return np.cov(a, b)[1, 0]/(np.std(a)*np.std(b))\n",
        "\n",
        "def r2(a, b):\n",
        "  return 1-np.sum((a-b)**2)/np.sum((a-np.mean(a))**2)\n",
        "\n",
        "def evaluate():\n",
        "  print(f'Number of timesteps analyzed = {len(all_means)}')\n",
        "  print(f'Maximum width of 95% CI = {np.max(np.abs(upper-lower)):.2f} {units[t]}')\n",
        "  print(f'Average width of 95% CI = {np.mean(np.abs(upper-lower)):.2f} {units[t]}')\n",
        "  print(f'Maximum punctual error magnitude for mean = {np.max(np.abs(all_truths-all_means)):.2f} {units[t]}')\n",
        "  print(f'Maximum punctual error magnitude for upper bound = {np.max(np.abs(all_truths-upper)):.2f} {units[t]}')\n",
        "  print(f'Maximum punctual error magnitude for lower bound = {np.max(np.abs(all_truths-lower)):.2f} {units[t]}')\n",
        "  acc = np.sum(all_truths[all_truths >= lower] <= upper[all_truths >= lower])/len(all_truths)\n",
        "  print(f'95% CI interval accuracy = {acc:.2%}')\n",
        "  print()\n",
        "\n",
        "  print('Evaluation for mean value:')\n",
        "  print(f'\\tRMSE = {rmse(all_truths, all_means):.2f} {units[t]}')\n",
        "  print(f'\\tMAE = {mae(all_truths, all_means):.2f} {units[t]}')\n",
        "  print(f'\\tCC = {cc(all_truths, all_means):.2%}')\n",
        "  print(f'\\tR2 = {r2(all_truths, all_means):.2%}')\n",
        "  print(f'\\tDTW = {dtw.distance_fast(all_truths, all_means):.2f}')\n",
        "  print()\n",
        "  print('Evaluation for upper bound:')\n",
        "  print(f'\\tRMSE = {rmse(all_truths, upper):.2f} {units[t]}')\n",
        "  print(f'\\tMAE = {mae(all_truths, upper):.2f} {units[t]}')\n",
        "  print(f'\\tCC = {cc(all_truths, upper):.2%}')\n",
        "  print(f'\\tR2 = {r2(all_truths, upper):.2%}')\n",
        "  print(f'\\tDTW = {dtw.distance_fast(all_truths, upper):.2f}')\n",
        "  print()\n",
        "  print('Evaluation for lower bound:')\n",
        "  print(f'\\tRMSE = {rmse(all_truths, lower):.2f} {units[t]}')\n",
        "  print(f'\\tMAE = {mae(all_truths, lower):.2f} {units[t]}')\n",
        "  print(f'\\tCC = {cc(all_truths, lower):.2%}')\n",
        "  print(f'\\tR2 = {r2(all_truths, lower):.2%}')\n",
        "  print(f'\\tDTW = {dtw.distance_fast(all_truths, lower):.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9S8CuRtN-g7"
      },
      "outputs": [],
      "source": [
        "x_axis = wind_data.loc[test_portion[0]:test_portion[-1]][offset:].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19eLds0o46Zp"
      },
      "outputs": [],
      "source": [
        "# Running in single execution mode for quick debugging.\n",
        "# Although performance metrics will resemble those obtained in proper sampling (N >= 10),\n",
        "# they will be slightly perturbed on account of statistical noise.\n",
        "# For proper runs, encapsulate in an external for loop guided by N, then compute the\n",
        "# corresponding temporal aggregations (mean, epistemic uncertainty and aleatoric uncertainty).\n",
        "all_means = []\n",
        "all_stds = []\n",
        "all_truths = []\n",
        "for batch in tqdm(test_set):\n",
        "  X, y = batch\n",
        "  all_truths += y.numpy().flatten().tolist()\n",
        "  prediction_distribution = model(X)\n",
        "  prediction_mean = prediction_distribution.mean().numpy().flatten().tolist()\n",
        "  all_means += prediction_mean\n",
        "  prediction_stdv = prediction_distribution.stddev().numpy().flatten()\n",
        "  all_stds += prediction_stdv.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Om6zWAOcDurx"
      },
      "outputs": [],
      "source": [
        "all_means=np.array(all_means).flatten()\n",
        "all_stds = np.array(all_stds).flatten()\n",
        "all_truths = destandardize(np.array(all_truths).flatten(), inpt=False)\n",
        "\n",
        "upper = destandardize((all_means + (1.96 * all_stds)).tolist(), inpt=False)\n",
        "lower = destandardize((all_means - (1.96 * all_stds)).tolist(), inpt=False)\n",
        "all_means= destandardize(all_means, inpt=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbWvIhZsEIqB"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(x_axis[len(x_axis)-len(all_truths):], all_truths, label='Observation')\n",
        "plt.plot(x_axis[len(x_axis)-len(all_truths):], all_means, linestyle='--', label='Predicted $\\\\mu$')\n",
        "plt.fill_between(x_axis[len(x_axis)-len(all_truths):], upper, lower, alpha=.25,\n",
        "                 color='orange', label='95% CI')\n",
        "plt.ylabel(f'${t_names[t]}$ ({units[t]})')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_nzphq12xco"
      },
      "outputs": [],
      "source": [
        "evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uwcu8bmfO5yG"
      },
      "source": [
        "## Kernel halting\n",
        "\n",
        "Uncomment `halt` (undefined function) for planned attended usage; comment it for safe, resource-effective unattended usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEVMehoJBdKs"
      },
      "outputs": [],
      "source": [
        "if save_model:\n",
        "  from google.colab import files\n",
        "  model.save_weights(f'solar_wind_{t}_{t_days}_model_for_plots.h5', overwrite=True) # save only the weights because Keras has issues saving full variational layers (Bayesian inference parameters are saved)\n",
        "  files.download(f'solar_wind_{t}_{t_days}_model_for_plots.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF4KC2A2FKh3"
      },
      "outputs": [],
      "source": [
        "if halt_at_end:\n",
        "  halt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_sCJnfwFHWk"
      },
      "outputs": [],
      "source": [
        "# Disconnect to prevent resource misuse\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}